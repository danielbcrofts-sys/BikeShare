max_models = 15,
seed = 521
)
dat_test <- vroom("test.csv")
dat_train <- vroom("train.csv")
dat_train <- dat_train %>%
mutate(
hour = hour(datetime),
dow = wday(datetime, label = TRUE)
)
dat_train <- dat_train %>%
select(-casual, -registered) %>%
mutate(count = log(count))
dat_test <- dat_test %>%
mutate(
hour = hour(datetime),
dow = wday(datetime, label = TRUE)
)
# Recipe
bike_recipe <- recipe(count ~ ., data = dat_train) %>%
# Fix weather 4 â†’ 3
step_mutate(
weather = ifelse(weather == 4, 3, weather),
weather = factor(weather),
season = factor(season),
holiday = factor(holiday),
workingday = factor(workingday)
) %>%
# Polynomial on hour to capture morning/evening peaks
step_poly(hour, degree = 3) %>%
# Interactions between hour and temperature
# Dummy code all nominal variables
step_dummy(all_nominal_predictors()) %>%
# Remove zero variance predictors
step_zv(all_predictors()) %>%
# Normalize numeric predictors
step_normalize(all_numeric_predictors())
# Prep and bake
prepped_recipe <- prep(bike_recipe)
baked_train <- bake(prepped_recipe, new_data = dat_train)
baked_test  <- bake(prepped_recipe, new_data = dat_test)
# Convert baked datasets to H2O frames
train_h2o <- as.h2o(baked_train)
test_h2o  <- as.h2o(baked_test)
# Run H2O AutoML on baked features
auto_model <- h2o.automl(
x = setdiff(names(baked_train), "count"),  # all predictors
y = "count",
training_frame = train_h2o,
max_runtime_secs = 600,
max_models = 15,
seed = 12
)
# Predict on test set
preds <- h2o.predict(auto_model@leader, newdata = test_h2o)
preds <- as.vector(preds)
preds <- exp(preds)     # back-transform log(count)
preds <- pmax(preds, 0)
# Kaggle submission
kaggle_submission <- tibble(
datetime = format(dat_test$datetime, "%Y-%m-%d %H:%M:%S"),
count = preds
)
vroom::vroom_write(kaggle_submission, "stacked_submission.csv", delim = ",")
dat_test <- vroom("test.csv")
dat_train <- vroom("train.csv")
# --- Prepare train/test explicitly (create features, then DROP raw hour) ---
dat_train_prep <- dat_train %>%
mutate(
hour      = hour(datetime),
dow       = wday(datetime, label = TRUE),
hour_sin  = sin(2 * pi * hour / 24),
hour_cos  = cos(2 * pi * hour / 24),
month     = month(datetime),
weather   = ifelse(weather == 4, 3, weather)  # optional collapse
) %>%
select(-casual, -registered, -hour) %>%   # drop raw hour BEFORE recipe
mutate(
count = log(count),                      # keep log target
season = factor(season),
holiday = factor(holiday),
workingday = factor(workingday),
dow = factor(dow),
month = factor(month),
weather = factor(weather)
)
dat_test_prep <- dat_test %>%
mutate(
hour      = hour(datetime),
dow       = wday(datetime, label = TRUE),
hour_sin  = sin(2 * pi * hour / 24),
hour_cos  = cos(2 * pi * hour / 24),
month     = month(datetime),
weather   = ifelse(weather == 4, 3, weather)
) %>%
select(-hour) %>%
mutate(
season = factor(season),
holiday = factor(holiday),
workingday = factor(workingday),
dow = factor(dow),
month = factor(month),
weather = factor(weather)
)
# --- Recipe (now safe: raw hour is already gone) ---
bike_recipe <- recipe(count ~ ., data = dat_train_prep) %>%
step_novel(all_nominal_predictors()) %>%      # handle unseen factor levels in test
step_dummy(all_nominal_predictors()) %>%      # you can remove this if you want H2O to handle factors natively
step_zv(all_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(bike_recipe)
baked_train <- bake(prepped_recipe, new_data = dat_train_prep)
baked_test  <- bake(prepped_recipe, new_data = dat_test_prep)
# --- Convert and run AutoML as before ---
train_h2o <- as.h2o(baked_train)
test_h2o  <- as.h2o(baked_test)
aml <- h2o.automl(
x = setdiff(names(baked_train), "count"),
y = "count",
training_frame = train_h2o,
max_runtime_secs = 600,
max_models = 15,
seed = 123
)
preds <- h2o.predict(aml@leader, newdata = test_h2o) %>%
as.vector() %>%
exp() %>%
pmax(0)
submission <- tibble(
datetime = format(dat_test$datetime, "%Y-%m-%d %H:%M:%S"),
count = preds
)
vroom::vroom_write(submission, "stacked_submission.csv", delim = ",")
dat_test <- vroom("test.csv")
dat_train <- vroom("train.csv")
# --- Prepare train/test explicitly (create features, then DROP raw hour) ---
dat_train_prep <- dat_train %>%
mutate(
hour      = hour(datetime),
dow       = wday(datetime, label = TRUE),
hour_sin  = sin(2 * pi * hour / 24),
hour_cos  = cos(2 * pi * hour / 24),
month     = month(datetime),
weather   = ifelse(weather == 4, 3, weather)  # optional collapse
) %>%
select(-casual, -registered, -hour) %>%   # drop raw hour BEFORE recipe
mutate(
count = log(count),                      # keep log target
season = factor(season),
holiday = factor(holiday),
workingday = factor(workingday),
dow = factor(dow),
month = factor(month),
weather = factor(weather)
)
dat_test_prep <- dat_test %>%
mutate(
hour      = hour(datetime),
dow       = wday(datetime, label = TRUE),
hour_sin  = sin(2 * pi * hour / 24),
hour_cos  = cos(2 * pi * hour / 24),
month     = month(datetime),
weather   = ifelse(weather == 4, 3, weather)
) %>%
select(-hour) %>%
mutate(
season = factor(season),
holiday = factor(holiday),
workingday = factor(workingday),
dow = factor(dow),
month = factor(month),
weather = factor(weather)
)
# --- Recipe (now safe: raw hour is already gone) ---
bike_recipe <- recipe(count ~ ., data = dat_train_prep) %>%
step_novel(all_nominal_predictors()) %>%      # handle unseen factor levels in test
step_dummy(all_nominal_predictors()) %>%      # you can remove this if you want H2O to handle factors natively
step_zv(all_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(bike_recipe)
baked_train <- bake(prepped_recipe, new_data = dat_train_prep)
baked_test  <- bake(prepped_recipe, new_data = dat_test_prep)
# --- Convert and run AutoML as before ---
train_h2o <- as.h2o(baked_train)
test_h2o  <- as.h2o(baked_test)
aml <- h2o.automl(
x = setdiff(names(baked_train), "count"),
y = "count",
training_frame = train_h2o,
max_runtime_secs = 1000,
max_models = 15,
seed = 1234
)
preds <- h2o.predict(aml@leader, newdata = test_h2o) %>%
as.vector() %>%
exp() %>%
pmax(0)
submission <- tibble(
datetime = format(dat_test$datetime, "%Y-%m-%d %H:%M:%S"),
count = preds
)
vroom::vroom_write(submission, "stacked_submission_new.csv", delim = ",")
dat_test <- vroom("test.csv")
dat_train <- vroom("train.csv")
# --- Prepare train/test explicitly (create features, then DROP raw hour) ---
dat_train_prep <- dat_train %>%
mutate(
hour      = hour(datetime),
dow       = wday(datetime, label = TRUE),
hour_sin  = sin(2 * pi * hour / 24),
hour_cos  = cos(2 * pi * hour / 24),
month     = month(datetime),
weather   = ifelse(weather == 4, 3, weather)  # optional collapse
) %>%
select(-casual, -registered, -hour) %>%   # drop raw hour BEFORE recipe
mutate(
count = log(count),                      # keep log target
season = factor(season),
holiday = factor(holiday),
workingday = factor(workingday),
dow = factor(dow),
month = factor(month),
weather = factor(weather)
)
dat_test_prep <- dat_test %>%
mutate(
hour      = hour(datetime),
dow       = wday(datetime, label = TRUE),
hour_sin  = sin(2 * pi * hour / 24),
hour_cos  = cos(2 * pi * hour / 24),
month     = month(datetime),
weather   = ifelse(weather == 4, 3, weather)
) %>%
select(-hour) %>%
mutate(
season = factor(season),
holiday = factor(holiday),
workingday = factor(workingday),
dow = factor(dow),
month = factor(month),
weather = factor(weather)
)
dat_train <- dat_train %>% mutate(weekend = ifelse(dow %in% c("Sat","Sun"), 1, 0))
dat_test  <- dat_test  %>% mutate(weekend = ifelse(dow %in% c("Sat","Sun"), 1, 0))
library(xgboost)
library(doParallel)
library(lubridate)
library(finetune)
library(lightgbm)
library(xgboost)
library(lightgbm)
library(xgboost)
install.packages("xgboost")
library(xgboost)
library(doParallel)
install.packages("doParallel")
install.packages("finetune")
library(xgboost)
library(doParallel)
library(lubridate)
library(finetune)
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(patchwork)
library(rpart)
library(ranger)
library(bonsai)
library(lightgbm)
library(xgboost)
library(doParallel)
library(lubridate)
library(finetune)
dat_train <- vroom::vroom("train.csv", show_col_types = FALSE) %>%
select(-casual, -registered) %>%
mutate(count = log1p(count))
dat_test <- vroom::vroom("./test.csv", show_col_types = FALSE)
#recipe
bike_recipe <- recipe(count ~ ., data = dat_train) %>%
step_mutate(datetime = as.POSIXct(datetime, tz = "UTC")) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(
weather = factor(weather, levels = c(1, 2, 3), labels = c("clear", "mist", "precip")),
season = factor(season, levels = c(1, 2, 3, 4), labels = c("spring", "summer", "fall", "winter")),
holiday = factor(holiday, levels = c(0, 1), labels = c("no", "yes")),
workingday = factor(workingday, levels = c(0, 1), labels = c("no", "yes"))
) %>%
step_time(datetime, features = c("hour"), keep_original_cols = TRUE) %>%
step_date(datetime, features = c("year", "dow", "month"), keep_original_cols = FALSE) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE)
# cookies in the oven
prepped_recipe <- prep(bike_recipe)
bake(prepped_recipe, new_data = dat_train)
#Boost model
xgb_spec <- boost_tree(
trees = 1000,
tree_depth = tune(), min_n = tune(),
loss_reduction = tune(), sample_size = tune(), mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("regression")
#workflow
xgb_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(xgb_spec)
# set up parallel processing
num_cores <- parallel::detectCores(logical = FALSE) - 1
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)
#tuning grid
xgb_grid <- grid_space_filling(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), trainData),
learn_rate(),
size = 30
)
#tuning grid
xgb_grid <- grid_space_filling(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dat_train),
learn_rate(),
size = 30
)
#cross-validation
folds <- vfold_cv(trainData, v = 5)
#cross-validation
folds <- vfold_cv(dat_train, v = 5)
# tune the model using the new grid
race_results <- tune_race_anova(
object = xgb_wf,
resamples = folds,
grid = xgb_grid,
metrics = metric_set(rmse),
control = control_race(verbose_elim = TRUE)
)
dat_train <- vroom::vroom("train.csv", show_col_types = FALSE) %>%
select(-casual, -registered) %>%
mutate(count = log1p(count))
dat_test <- vroom::vroom("./test.csv", show_col_types = FALSE)
#recipe
bike_recipe <- recipe(count ~ ., data = dat_train) %>%
step_mutate(datetime = as.POSIXct(datetime, tz = "UTC")) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(
weather = factor(weather, levels = c(1, 2, 3), labels = c("clear", "mist", "precip")),
season = factor(season, levels = c(1, 2, 3, 4), labels = c("spring", "summer", "fall", "winter")),
holiday = factor(holiday, levels = c(0, 1), labels = c("no", "yes")),
workingday = factor(workingday, levels = c(0, 1), labels = c("no", "yes"))
) %>%
step_time(datetime, features = c("hour"), keep_original_cols = TRUE) %>%
step_date(datetime, features = c("year", "dow", "month"), keep_original_cols = FALSE) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE)
# cookies in the oven
prepped_recipe <- prep(bike_recipe)
bake(prepped_recipe, new_data = dat_train)
#Boost model
xgb_spec <- boost_tree(
trees = 1000,
tree_depth = tune(), min_n = tune(),
loss_reduction = tune(), sample_size = tune(), mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("regression")
#workflow
xgb_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(xgb_spec)
# set up parallel processing
num_cores <- parallel::detectCores(logical = FALSE) - 1
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)
#tuning grid
xgb_grid <- grid_space_filling(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dat_train),
learn_rate(),
size = 30
)
#cross-validation
folds <- vfold_cv(dat_train, v = 5)
# tune the model using the new grid
race_results <- tune_race_anova(
object = xgb_wf,
resamples = folds,
grid = xgb_grid,
metrics = metric_set(rmse),
control = control_race(verbose_elim = TRUE)
)
stopCluster(cl)
best_xgb <- select_best(race_results, metric = "rmse")
# Finalize workflow
final_wf <- finalize_workflow(
xgb_wf,
best_xgb
)
# fit the model to the training data
final_fit <- fit(final_wf, data = trainData)
dat_train <- vroom::vroom("train.csv", show_col_types = FALSE) %>%
select(-casual, -registered) %>%
mutate(count = log1p(count))
dat_test <- vroom::vroom("./test.csv", show_col_types = FALSE)
#recipe
bike_recipe <- recipe(count ~ ., data = dat_train) %>%
step_mutate(datetime = as.POSIXct(datetime, tz = "UTC")) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(
weather = factor(weather, levels = c(1, 2, 3), labels = c("clear", "mist", "precip")),
season = factor(season, levels = c(1, 2, 3, 4), labels = c("spring", "summer", "fall", "winter")),
holiday = factor(holiday, levels = c(0, 1), labels = c("no", "yes")),
workingday = factor(workingday, levels = c(0, 1), labels = c("no", "yes"))
) %>%
step_time(datetime, features = c("hour"), keep_original_cols = TRUE) %>%
step_date(datetime, features = c("year", "dow", "month"), keep_original_cols = FALSE) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE)
# cookies in the oven
prepped_recipe <- prep(bike_recipe)
bake(prepped_recipe, new_data = dat_train)
#Boost model
xgb_spec <- boost_tree(
trees = 1000,
tree_depth = tune(), min_n = tune(),
loss_reduction = tune(), sample_size = tune(), mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("regression")
#workflow
xgb_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(xgb_spec)
# set up parallel processing
num_cores <- parallel::detectCores(logical = FALSE) - 1
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)
#tuning grid
xgb_grid <- grid_space_filling(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), dat_train),
learn_rate(),
size = 30
)
#cross-validation
folds <- vfold_cv(dat_train, v = 5)
# tune the model using the new grid
race_results <- tune_race_anova(
object = xgb_wf,
resamples = folds,
grid = xgb_grid,
metrics = metric_set(rmse),
control = control_race(verbose_elim = TRUE)
)
stopCluster(cl)
best_xgb <- select_best(race_results, metric = "rmse")
# Finalize workflow
final_wf <- finalize_workflow(
xgb_wf,
best_xgb
)
# fit the model to the training data
final_fit <- fit(final_wf, data = dat_train)
predictions <- predict(final_fit, new_data = dat_test)
# Create the submission file
submission <- predictions %>%
mutate(.pred = pmax(0, round(expm1(.pred)))) %>% # Inverse log transform
rename(count = .pred) %>%
bind_cols(dat_test %>% select(datetime)) %>%
select(datetime, count) %>%
mutate(datetime = format(as.POSIXct(datetime, tz = "UTC"), "%Y-%m-%d %H:%M:%S"))
vroom::vroom_write(submission, "submission_final.csv", delim = ",")
1 - pt(2, df = 5)
pt(-2, df = 5)
pt(2, df = 5) - pt(-2, df = 5)
1 - pt(2, df = 5)
pt(-2, df = 5)
pt(2, df = 5) - pt(-2, df = 5)
library(tidyverse)
# Read in the data outputted by the SURVEY program
surveydata <- read.table(file = "SRSHW4_234", sep = "\t", skip = 1,
stringsAsFactors = FALSE)
library(tidyverse)
# Read in the data outputted by the SURVEY program
surveydata <- read.table(file = "SRSHW4_234", sep = "\t", skip = 1,
stringsAsFactors = FALSE)
# Read in the data outputted by the SURVEY program
surveydata <- read.table(file = "SRSHW4_234.txt", sep = "\t", skip = 1,
stringsAsFactors = FALSE)
cost_of_survey <- surveydata[nrow(surveydata),] # Save cost of survey
# Data cleaning
require(stringr)
surveylist <- str_extract_all(surveydata$V1, "\\d+")
survey_data_frame <- data.frame(surveylist[[1]])
for (i in 2:(length(surveylist) - 1)) {
survey_data_frame <- cbind(survey_data_frame, data.frame(surveylist[[i]]))
}
survey_data_frame <- t(survey_data_frame) # Transpose data.frame
rownames(survey_data_frame) <- NULL
survey_data_frame <- data.frame(survey_data_frame)
